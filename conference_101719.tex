\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{LRU*\\
{\footnotesize \textsuperscript{*}}
\thanks{}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Penghui Zhu}
	\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
		\textit{name of organization (of Aff.)}\\
		City, Country \\
		email address or ORCID}
}

\maketitle

\begin{abstract}
	In the contemporary computer system world, Least Recently Used (LRU) algorithm has been a famous and widely adopted cache replacement policy that offers good object hit ratio (OHR) and performance. It maintains the recency information about the objects stored and do replacement based on that knowledge. However, the algorithm is totally based on recency and has not taken the size of the objects into consideration. This can lead to large cache size in memory, causing problems and posing challenges in scenarios where memory is limited. In this paper, a series of modifications to the basic LRU algorithm are introduced, aiming to reduce the overall memory consumption while still reaching a good OHR and performance while keep an eye on the object size when adopting. The Size-Aware CLOCK (SA-CLOCK) algorithm is proved through experiments based on real-world traces, showing that it provides a more memory-efficient cache replacement policy, opening the possibility of maintaining good OHR while reducing memory usage.
\end{abstract}

\begin{IEEEkeywords}
	cache, LRU, memory-efficient, size-aware
\end{IEEEkeywords}

\section{Introduction}
In the realm of memory management and data caching, the Least Recently Used (LRU) algorithm has long stood as a cornerstone for optimizing data retrieval in various computing systems. Specifically designed for in-memory cache systems, LRU employs a mechanism that prioritizes recently accessed data, thereby enhancing cache hit rates and consequently improving overall system performance. However, there are many scenarios in contemporary industry where memory resources are constrained. In such instances, keeping a balance between maintaining high cache hit rates and minimizing the memory consumption becomes imperative. Traditional recency based algorithm like LRU falls short in dealing with memory limitations. Some modified recency based algorithms like CLOCK pays more attention to memory overhead by simplifying LRU, yet they still neglects object sizes within the cache.
This paper delves into the evolving landscape of recency based caching strategies, motivated by the need to optimize memory usage while preserving cache efficiency. This work focuses on memory size limitations and propose a new recency based cache policy to adapt to these constraints. One notable oversight in LRU and CLOCK is the neglect of the individual sizes of objects within the cache. Recognizing this limitation, our proposed algorithm seeks to rectify this gap by introducing a novel approach that not only upholds cache hit rates but also strategically attends to the size of objects stored in the cache.

In the subsequent sections, we present an in-depth exploration of the limitations of traditional algorithms and articulate the design principles and mechanisms behind our newly proposed algorithm. Through meticulous consideration of object sizes, our algorithm aims to demonstrate a reduction in overall memory consumption while keeping cache hit rates at par with or potentially surpassing existing approaches. This paradigm shift represents a significant stride towards more resource-efficient and effective memory management in contemporary computing environments.

The rest of the paper is organized as follows. Section 2 presents the background and motivations. Section 3 discusses some related work in this area and address their differences from our approach. Section 4 elaborates the principle of SA-CLOCK. Section 5 provides evaluation for SA-CLOCK. Section 6 summarizes some related works. The last section concludes this work.

\section{Background and Motivation}

\subsection{Least Recently Used}
The Least Recently Used (LRU) cache replacement policy is a foundational concept in computer science, vital for optimizing cache system performance. Caches, high-speed memory storage units, store frequently accessed data to expedite retrieval from slower, main memory. LRU efficiently manages cache contents by prioritizing the retention of recently accessed data.
LRU operates on the principle of discarding the least recently used items in the cache when space is needed for new data. This strategy is grounded in the concept of temporal locality, assuming that recently accessed items are more likely to be accessed again soon. LRU employs heuristics based on historical usage patterns to make informed decisions about cache management.
Common LRU implementations involve maintaining a linked list or a similar data structure to track the order in which items were accessed. When an item is accessed, it is moved to the front of the list, signifying recent use. Eviction occurs by removing the item at the list's end when the cache reaches capacity. Alternatively, counters or timestamps associated with each item can be used to identify the least recently used item when eviction is necessary.
In implementing the LRU cache policy, a hash map is commonly used to efficiently reference objects in the linked list. This hash map acts as a lookup table, associating each item with its position in the list. However, this approach introduces potential memory overhead as the hash map itself requires extra memory for mapping information. In scenarios with numerous cached items, managing this additional memory becomes crucial to strike a balance between performance gains and resource utilization.
Moreover, LRU also has no adoption or eviction policies related to the objects' size. This means that LRU can adopt very large objects into cache, resulting in the surge of the memory consumption. Given a limited amount of memory space, LRU would hurt other applications for taking up too much memory.

\subsection{CLOCK}
CLOCK is another widely adopted cache replacement policy. CLOCK simplifies memory management by assigning a single-byte flag to each cached object, eliminating the need for complex hashmap structures. The algorithm builds upon LRU principles but distinguishes itself through a circular buffer mechanism, enhancing efficiency without compromising performance.
CLOCK's core principle involves a circular buffer and a clock hand that moves sequentially through cached objects. When an object is accessed, its flag is set, and the clock hand advances. If the hand encounters a flagged object, it resets the flag. This modification, combined with a straightforward replacement strategy based on the clock hand's position, preserves the essence of recency based eviction without the need for extensive tracking.
CLOCK achieves remarkable efficiency by utilizing only a single byte per cached object for the flag, in stark contrast to LRU's reliance on hashmaps. The circular buffer's minimal extra memory requirement ensures a lightweight footprint. However, CLOCK ignores object characteristics and sizes in its decision-making process. Therefore, CLOCK still suffers from the possibility of taking up too much memory space despite its relatively simpleness compared to LRU.

\section{Related Works}
In this section, we briefly review recent work related to our design and highlight differences with our approach.

\subsection{TBF}
TBF is new RAM-frugal cache replacement policy that approximates the least-recently-used (LRU) policy. It uses two in-memory Bloom sub-filters (TBF) for maintaining the recency information and leverages an on-flash key–value store to cache objects. TBF requires only one byte of RAM per cached object, making it suitable for implementing very large flash-based caches. The two bloom filters here is used to realize the delete operation as one is discarded periodically. TBF is evaluated through simulation on traces from several block stores and key–value stores, as well as using the Yahoo! Cloud Serving Benchmark in a real system implementation. The results show that TBF achieves cache hit rate and operations per second comparable to those of LRU in spite of its much smaller memory requirements.

\subsection{Me-CLOCK}
Me-CLOCK stands for Memory-Efficient CLOCK and it targets to reduce the memory= overhead introduced by the replacement policy of SSD-based cache. It proposes a memory-efficient framework which keeps most data structures in SSD while just leaving the memory efficient data structure in main memory.
The memory efficient data structure here refers to a new kind of bloom filter introduced by this paper. Unlike a traditional bloom filter, the new bloom filter supports element deletion and it takes over the responsibility of keeping the reused flag, indicating whether a page has been frequently accessed. The framework can be used to implement any LRU-based replacement policies under negligible memory overhead. The evaluation shows that its memory overhead is 10 times less that that introduced by traditional manners such as LRU or CLOCK.
Me-CLOCK is similar to TBF, but it is better in the way that it can be applied to algorithms other than LRU. It also has less memory overhead because it only uses 1 bloom filter instead of 2.

Both TBF and Me-CLOCK are designed for flash-based cache and have indeed reduced the memory overhead, but they do not pay any attention to the objects themselves. Therefore, they could lead to a bigger memory consumption under the same Object Hit Ratio.

\subsection{AdaptSize}
This paper proposed a new cache admission policy called AdaptSize. It is the first caching system to uses a size-aware admission policy that is continuously adapted to the request traffic. There are two variants proposed. AdaptSize(Chance) probabilistically decides whether or not to admit an object into the cache with the admission probability equal to $e^{-\frac{\text{size}}{c}}$, where c is a tunable parameter. AdaptSize(Threshold) admits an object if its size is below a tunable parameter c. The threshold and its parameters are constantly updated as the requests come in. AdaptSize is evaluated on production request traces and the results show that AdaptSize indeed improves the Object Hit Ratio.
AdaptSize is differ from other policies due to its attention to object size. It chooses to use the more complicated AdaptSize(Chance) while the results shows that there is actually very little different between it and AdaptSize(Threshold). The author evaluated AdaptSize by combining it with the Varnish Cache, but did not tried to combine it with the LRU policy.

\section{Size-Aware CLOCK}
\subsection{Overview}
Size-Aware CLOCK is an innovative recency based cache replacement strategy that builds upon the established CLOCK algorithm while introducing a size-aware dimension to enhance caching efficiency. In contrast to traditional approaches, SA-CLOCK dynamically evaluates the size of each incoming object against a threshold. This criterion ensures that only objects smaller than the specified threshold are permitted entry into the cache. By incorporating this size-aware filtering mechanism, SA-CLOCK aims to optimize cache memory consumption by tailoring its decisions to the individual characteristics of stored objects.

The primary motivation behind SA-CLOCK comes from the diverse object sizes encountered in modern computing environments. This strategy recognizes that smaller objects may benefit more from frequent caching, and by selectively admitting them into the cache, it seeks to strike a balance between maximizing cache hits and minimizing space usage. Through this approach, SA-CLOCK not only inherits the strengths of the CLOCK algorithm, such as recency knowledge, simplicity and low computational overhead, but also introduces a nuanced consideration of object size.

SA-CLOCK introduces a dynamic threshold mechanism that continuously adapts to incoming inputs, enhancing its cache replacement strategy. This dynamic threshold is fine-tuned in real-time through the utilization of an exponential smoothing algorithm. Unlike static thresholds, SA-CLOCK's dynamic approach allows it to autonomously respond to changes in the characteristics of incoming objects, providing a more responsive and adaptive caching strategy. The incorporation of the exponential smoothing algorithm ensures that the threshold evolves gradually, striking a balance between responsiveness to recent changes and stability against short-term fluctuations. This dynamic and intelligent threshold adjustment mechanism equips SA-CLOCK to optimize cache utilization in varying workloads, making it a versatile and efficient solution for dynamic computing environments.

In summary, SA-CLOCK incorporates size awareness on top of recency information to refine the process of object admission into the cache. By combining the established principles of the CLOCK algorithm with a novel size-based criterion, SA-CLOCK endeavors to enhance overall cache performance, catering to the unique demands posed by varying object sizes in contemporary computing environments.

\subsection{Admission Formula}
As it is discussed in the previous section, our Size-Aware Clock makes admission decisions based on object size. The admission judgement can be expressed using the following formula.
\[
\text{admit if } \text{size} < \text{threshold} \text{ c}
\]
Here threshold c is a variable that is updated real-time, based on the size of the input objects. We will discuss this in detail later.

\subsection{Tunning the Size Threshold}
In contrast to a fixed threshold, the Size-Aware CLOCK caching strategy employs a dynamic threshold that undergoes continuous adjustments in response to the varying sizes of input objects. The dynamic nature of this threshold is achieved through the implementation of an exponential smoothing approach. This methodology ensures that the threshold's adaptation to a given input object is not abrupt but rather reflects a nuanced, gradual response.

Utilizing the exponential smoothing technique affords us the capability to assign higher significance to recent input data, recognizing its potential impact on system dynamics. Simultaneously, it diminishes the importance of older data progressively over time, allowing the caching mechanism to stay attuned to evolving patterns and trends in the input dataset.

By embracing this adaptive approach, our Size-Aware CLOCK not only accommodates fluctuations in the sizes of input objects but also avoids extreme responses, such as outright rejection of all objects upon size increase or indiscriminate admission of all objects following a size reduction. This adaptability ensures a more nuanced and responsive caching system, capable of optimizing performance across a spectrum of input scenarios. 

1. Exponential smoothing formula
2. compare different exponential smoothing parameters
3. prove that exponential smoothing is indispensable 


\section{Evaluation}
\subsection{Evaluation methodology}


\section*{References}

Please number citations consecutively within brackets \cite{b1}. The
sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference
number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at
the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

Number footnotes separately in superscripts. Place the actual footnote at
the bottom of the column in which it was cited. Do not put footnotes in the
abstract or reference list. Use letters for table footnotes.

Unless there are six authors or more give all authors' names; do not use
``et al.''. Papers that have not been published, even if they have been
submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers
that have been accepted for publication should be cited as ``in press'' \cite{b5}.
Capitalize only the first word in a paper title, except for proper nouns and
element symbols.

For papers published in translation journals, please give the English
citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
	\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
	\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
	\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
	\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
	\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
	\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
	\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}
\vspace{12pt}
\color{red}
IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
