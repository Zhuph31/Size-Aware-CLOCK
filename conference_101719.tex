\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[export]{adjustbox} 
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Achieving Low Memory Usage With Size-Aware Recency Based Cache*\\
{\footnotesize \textsuperscript{*}}
\thanks{}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Penghui Zhu}
	\IEEEauthorblockA{\textit{Department of Electric and Computer Enginerring} \\
		\textit{University of Toronto}\\
		Toronto, Canada \\
		penghui.zhu@mail.utoronto.ca}
}

\maketitle

\begin{abstract}
	In the contemporary computer system world, recency based cache policy is a topic that is gaining considerable attention. Among all the recency based policies, Least Recently Used (LRU) algorithm has been a famous and widely adopted cache replacement policy that offers good object hit ratio (OHR) and performance. It maintains the recency information about the objects stored and do replacement based on that knowledge. However, the algorithm, like many other recency based algorithm, makes its replacement decisions totally based on recency and has not taken the size of the objects into consideration. This can lead to large cache size in memory, causing problems and posing challenges in scenarios where memory is limited. In this paper, a series of modifications to the basic recency based algorithm are introduced, aiming to reduce the overall memory consumption. We target at proposing a new recency based cache policy, which is capable of reaching a good OHR and performance while keep an eye on the object size when adopting. The Size-Aware CLOCK (SA-CLOCK) algorithm is proved through experiments based on real-world traces, showing that it provides a more memory-efficient cache replacement policy, opening the possibility of maintaining good OHR while reducing memory usage.
\end{abstract}

\begin{IEEEkeywords}
	cache, LRU, CLOCK, memory-efficient, size-aware
\end{IEEEkeywords}

\section{Introduction}
In the realm of memory management and data caching, the Least Recently Used (LRU) algorithm has long stood as a cornerstone for optimizing data retrieval in various computing systems. Specifically designed for in-memory cache systems, LRU employs a mechanism that prioritizes recently accessed data, thereby enhancing cache hit rates and consequently improving overall system performance. However, there are many scenarios in contemporary industry where memory resources are constrained. In such instances, keeping a balance between maintaining high cache hit rates and minimizing the memory consumption becomes imperative. Traditional recency based algorithm like LRU falls short in dealing with memory limitations. Some modified recency based algorithms like CLOCK pays more attention to memory overhead by simplifying LRU, yet they still neglects object sizes within the cache.

This paper delves into the evolving landscape of recency based caching strategies, motivated by the need to optimize memory usage while preserving cache efficiency. This work focuses on memory size limitations and propose a new recency based cache policy to adapt to these constraints. One notable oversight in LRU and CLOCK is the neglect of the individual sizes of objects within the cache. Recognizing this limitation, our proposed algorithm seeks to rectify this gap by introducing a novel approach that not only upholds cache hit rates but also strategically attends to the size of objects stored in the cache.

In the subsequent sections, we present an in-depth exploration of the limitations of traditional algorithms and articulate the design principles and mechanisms behind our newly proposed algorithm. Through meticulous consideration of object sizes, our algorithm aims to demonstrate a reduction in overall memory consumption while keeping cache hit rates at par with existing approaches. 

The rest of the paper is organized as follows. Section 2 presents the background and motivations. Section 3 discusses some related work in this area and address their differences from our approach. Section 4 elaborates the principle of SA-CLOCK. Section 5 articulate the evaluation methodology for SA-CLOCK. Section 6 provides the experiment results and related analysis. Section 7 discusses the future directions for improvements. The last section concludes this work.

\section{Background and Motivation}

\subsection{Least Recently Used}
The Least Recently Used (LRU) cache replacement policy is a foundational concept in computer science, vital for optimizing cache system performance. Caches, high-speed memory storage units, store frequently accessed data to expedite retrieval from slower, main memory. LRU efficiently manages cache contents by prioritizing the retention of recently accessed data.
LRU operates on the principle of discarding the least recently used items in the cache when space is needed for new data. This strategy is grounded in the concept of temporal locality, assuming that recently accessed items are more likely to be accessed again soon. LRU employs heuristics based on historical usage patterns to make informed decisions about cache management.
Common LRU implementations involve maintaining a linked list or a similar data structure to track the order in which items were accessed. When an item is accessed, it is moved to the front of the list, signifying recent use. Eviction occurs by removing the item at the list's end when the cache reaches capacity. Alternatively, counters or timestamps associated with each item can be used to identify the least recently used item when eviction is necessary.
In implementing the LRU cache policy, a hash map is commonly used to efficiently reference objects in the linked list. This hash map acts as a lookup table, associating each item with its position in the list. However, this approach introduces potential memory overhead as the hash map itself requires extra memory for mapping information. In scenarios with numerous cached items, managing this additional memory becomes crucial to strike a balance between performance gains and resource utilization.
Moreover, LRU also has no adoption or eviction policies related to the objects' size. This means that LRU can adopt very large objects into cache, resulting in the surge of the memory consumption. Given a limited amount of memory space, LRU would hurt other applications for taking up too much memory.

\subsection{CLOCK}
CLOCK is another widely adopted cache replacement policy. CLOCK simplifies memory management by assigning a single-byte flag to each cached object, eliminating the need for complex hashmap structures. The algorithm builds upon LRU principles but distinguishes itself through a circular buffer mechanism, enhancing efficiency without compromising performance.
CLOCK's core principle involves a circular buffer and a clock hand that moves sequentially through cached objects. When an object is accessed, its flag is set, and the clock hand advances. If the hand encounters a flagged object, it resets the flag. This modification, combined with a straightforward replacement strategy based on the clock hand's position, preserves the essence of recency based eviction without the need for extensive tracking.
CLOCK achieves remarkable efficiency by utilizing only a single byte per cached object for the flag, in stark contrast to LRU's reliance on hashmaps. The circular buffer's minimal extra memory requirement ensures a lightweight footprint. However, CLOCK ignores object characteristics and sizes in its decision-making process. Therefore, CLOCK still suffers from the possibility of taking up too much memory space despite its relatively simpleness compared to LRU.

\section{Related Works}
In this section, we briefly review recent work related to our design and highlight differences with our approach.

\subsection{TBF}
TBF is new RAM-frugal cache replacement policy that approximates the least-recently-used (LRU) policy. It uses two in-memory Bloom sub-filters (TBF) for maintaining the recency information and leverages an on-flash key–value store to cache objects. TBF requires only one byte of RAM per cached object, making it suitable for implementing very large flash-based caches. The two bloom filters here is used to realize the delete operation as one is discarded periodically. TBF is evaluated through simulation on traces from several block stores and key–value stores, as well as using the Yahoo! Cloud Serving Benchmark in a real system implementation. The results show that TBF achieves cache hit rate and operations per second comparable to those of LRU in spite of its much smaller memory requirements.

\subsection{Me-CLOCK}
Me-CLOCK stands for Memory-Efficient CLOCK and it targets to reduce the memory= overhead introduced by the replacement policy of SSD-based cache. It proposes a memory-efficient framework which keeps most data structures in SSD while just leaving the memory efficient data structure in main memory.
The memory efficient data structure here refers to a new kind of bloom filter introduced by this paper. Unlike a traditional bloom filter, the new bloom filter supports element deletion and it takes over the responsibility of keeping the reused flag, indicating whether a page has been frequently accessed. The framework can be used to implement any LRU-based replacement policies under negligible memory overhead. The evaluation shows that its memory overhead is 10 times less that that introduced by traditional manners such as LRU or CLOCK.
Me-CLOCK is similar to TBF, but it is better in the way that it can be applied to algorithms other than LRU. It also has less memory overhead because it only uses 1 bloom filter instead of 2.

Both TBF and Me-CLOCK are designed for flash-based cache and have indeed reduced the memory overhead, but they do not pay any attention to the objects themselves. Therefore, they could lead to a bigger memory consumption under the same Object Hit Ratio.

\subsection{AdaptSize}
This paper proposed a new cache admission policy called AdaptSize. It is the first caching system to uses a size-aware admission policy that is continuously adapted to the request traffic. There are two variants proposed. AdaptSize(Chance) probabilistically decides whether or not to admit an object into the cache with the admission probability equal to $e^{-\frac{\text{size}}{c}}$, where c is a tunable parameter. AdaptSize(Threshold) admits an object if its size is below a tunable parameter c. The threshold and its parameters are constantly updated as the requests come in. AdaptSize is evaluated on production request traces and the results show that AdaptSize indeed improves the Object Hit Ratio.
AdaptSize is differ from other policies due to its attention to object size. It chooses to use the more complicated AdaptSize(Chance) while the results shows that there is actually very little different between it and AdaptSize(Threshold). The author evaluated AdaptSize by combining it with the Varnish Cache, but did not tried to combine it with the LRU policy.

\section{Size-Aware CLOCK}
\subsection{Overview}
Size-Aware CLOCK is an innovative recency based cache replacement strategy that builds upon the established CLOCK algorithm while introducing a size-aware dimension to enhance caching efficiency. In contrast to traditional approaches, SA-CLOCK dynamically evaluates the size of each incoming object against a threshold. This criterion ensures that only objects smaller than the specified threshold are permitted entry into the cache. By incorporating this size-aware filtering mechanism, SA-CLOCK aims to optimize cache memory consumption by tailoring its decisions to the individual characteristics of stored objects.

The primary motivation behind SA-CLOCK comes from the diverse object sizes encountered in modern computing environments. This strategy recognizes that smaller objects may benefit more from frequent caching, and by selectively admitting them into the cache, it seeks to strike a balance between maximizing cache hits and minimizing space usage. Through this approach, SA-CLOCK not only inherits the strengths of the CLOCK algorithm, such as recency knowledge, simplicity and low computational overhead, but also introduces a nuanced consideration of object size.

SA-CLOCK introduces a dynamic threshold mechanism that continuously adapts to incoming inputs, enhancing its cache replacement strategy. This dynamic threshold is fine-tuned in real-time through the utilization of an exponential smoothing algorithm. Unlike static thresholds, SA-CLOCK's dynamic approach allows it to autonomously respond to changes in the characteristics of incoming objects, providing a more responsive and adaptive caching strategy. The incorporation of the exponential smoothing algorithm ensures that the threshold evolves gradually, striking a balance between responsiveness to recent changes and stability against short-term fluctuations. This dynamic and intelligent threshold adjustment mechanism equips SA-CLOCK to optimize cache utilization in varying workloads, making it a versatile and efficient solution for dynamic computing environments.

In summary, SA-CLOCK incorporates size awareness on top of recency information to refine the process of object admission into the cache. By combining the established principles of the CLOCK algorithm with a novel size-based criterion, SA-CLOCK endeavors to enhance overall cache performance, catering to the unique demands posed by varying object sizes in contemporary computing environments.

\subsection{Admission Formula}
As it is discussed in the previous section, our Size-Aware Clock makes admission decisions based on object size. The admission judgement can be expressed using the following formula.
\[
\text{admit if } \text{size} < \text{threshold} \text{ c}
\]
Here threshold c is a variable that is updated real-time, based on the size of the input objects. We will discuss this in detail later.

\subsection{Tunning the Size Threshold}
In contrast to a fixed threshold, the Size-Aware CLOCK caching strategy employs a dynamic threshold that undergoes continuous adjustments in response to the varying sizes of input objects. The dynamic nature of this threshold is achieved through the implementation of an exponential smoothing approach. This methodology ensures that the threshold's adaptation to a given input object is not abrupt but rather reflects a nuanced, gradual response.

Utilizing the exponential smoothing technique affords us the capability to assign higher significance to recent input data, recognizing its potential impact on system dynamics. Simultaneously, it diminishes the importance of older data progressively over time, allowing the caching mechanism to stay attuned to evolving patterns and trends in the input dataset.

By embracing this adaptive approach, our Size-Aware CLOCK not only accommodates fluctuations in the sizes of input objects but also avoids extreme responses, such as outright rejection of all objects upon size increase or indiscriminate admission of all objects following a size reduction. This adaptability ensures a more nuanced and responsive caching system, capable of optimizing performance across a spectrum of input scenarios. 

The exponential smoothing formula is as follow.
\[
\text{threshold}_{\text{new}} = a \cdot \text{input} + (1 - a) \cdot \text{threshold}_{\text{old}}
\]

Every time we performed an operation on the cache, the cache will update the threshold using the corresponding value's size. With this formula, the threshold will keep adapting to reflect a useful threshold that can provide a good distinctiveness.
Within the formula, the variable a is a parameter that needs to be given by the user of the Size-Aware CLOCK. We will delve into the find-tunning of this parameter in later sections.



\section{Evaluation Methodology}
In this section, we will discuss the evaluation methodology, dataset used, and competitors for our Size-Aware CLOCK.

\subsection{Performance Evaluation}
\subsubsection{Compared Methods}
To maintain parity in our evaluation, we rigorously implemented alternative cache policies for comparison. This strategic approach ensures that any observed differences in performance can be confidently attributed to the unique characteristics of the Size-Aware CLOCK policy, rather than variations in the evaluation methodologies.

Here we have chosen to implement LRU and CLOCK as opponents of our Size-Aware CLOCK. LRU is chosen because of it is the most famous and widely adopted recency based cache policy. CLOCK is chosen for the same reason, but also for it is the base of our Size-Aware CLOCK, which can serve as a excellent contrast to show the difference made by adding a size-aware layer when adopting objects into cache. Our evaluation criteria encompass a range of metrics commonly used to gauge cache performance, including hit rate, miss rate, and overall cache efficiency. By employing a multifaceted evaluation framework, we aim to provide a comprehensive view of how the Size-Aware CLOCK policy performs under various conditions and workloads.

\subsubsection{Real Wold Traces Based Evaluation}
We conducted a comprehensive evaluation of the Size-Aware CLOCK cache policy using real-world trace-based data. The implementation of Size-Aware CLOCK is robust and complete, and we took deliberate measures to ensure a fair and equitable evaluation by comparing its performance with other implemented cache policies.

In the evaluation process, we employed real-world trace-based data. The real world traces are published by Microsoft Research Cambridge and they are 1 week block I/O traces of enterprise servers at Microsoft. Therefore, this data represents the actual workload that the Microsoft research team are using, and can prove that the evaluation process's authenticity and can give a comprehensive test of our Size-Aware CLOCK, enhancing the relevance of our findings to real-world scenarios.

\subsubsection{Parameter Tunning}
In addition to assessing the Size-Aware CLOCK algorithm's performance, we delve into the specifics of its implementation. This includes a detailed exploration of any adjustable parameters or thresholds and an analysis of how these configurations impact the overall effectiveness of the cache policy.

% \subsubsection{Adaptability}
% Our study extends beyond a mere comparison of algorithms; we scrutinize the adaptability of the Size-Aware CLOCK policy under different scenarios and varying workload conditions. Specifically, we create a scenario where the input objects' sizes are changed drastically. This approach allows us to evaluate Size-Aware CLOCK's flexibility and responsiveness to changes in data access patterns and fluctuations in data size, which is one the most important targets that we designed the policies. By comparing Size-Aware CLOCK under this kind of scenario, we can easily tell the difference made by the size-aware layer and show that whether Size-Aware CLOCK has made its goals.
% 1. add the experiment setup details here.


\section{Evaluation Results}
In this section, we will show the evaluation results of Size-Aware CLOCK, as well as giving detailed explanation to the results.

\subsection{Performance Evaluation}
In this section, we focus on the performance of our Size-Aware CLOCK, including aspects like object hit ratio, performance respect to time cost, and memory consumption.

\subsubsection{Experiments Setup}
As mentioned in previous sections, we use real world traces posted by Microsoft Research Team as the input. For each item in the trace, a string with the specified length will be generated into a in-memory data structure, which serves as a global storage. In the meantime, the file name and the offset in the file combined are used as the key, which is later used for searching this object in the cache and the storage. 
When the experiments start, we start to range over the items in the traces and for each item, a get request is generated to try to get the result from the cache. If the get request succeeds, we will move onto the next item. Otherwise, we will get the real object from the global storage and generate an add request to try to add the object into the cache. For LRU and CLOCK, this object should be added into the cache, while SA-CLOCK will make judgement base on the size of the object.
For each experiment, we tried different combination of storage size and cache size. However, the results do not vary too much, therefore we only present some of the results as they can well represent the evaluation outcomes. Here we choose a storage size of 1,000,000 and a cache size of 1,000.

\subsubsection{Cache Miss Count}
For a cache policy, the most important aspect is its ability to cache objects, which can be reflected by Object Hit Ratio. Here we record the number of cache miss during the experiment and calculate the Miss Ratio. Figure 1 shows the results for LRU, CLOCK and our SA-CLOCK. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{pic/1000000-1000-miss}
    \caption{Object Miss Ratio}
    % \label{fig:sample}
\end{figure}

From the figure you can see that our SA-CLOCK is slightly worse than LRU and CLOCK, with about 2.4 percent. This shows that our SA-CLOCK is worse than the other two baseline policies respect to object hit.
1. conduct more experiment to answer this question
2. say that the decline is very small and could possibly be ignored


\subsubsection{Time Cost}
We also pay attention to the time cost of the whole experiment process, which is the time used to send the 1,000,000 get requests and possible add requests. One thing worth noticing is that with this evaluation strategy, there will be more add requests if there are more misses, which means that our SA-CLOCK actually has an inherent disadvantage in the test of time cost. Figure 2 shows the time used by each policy in milliseconds.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{pic/1000000-1000-tc}
    \caption{Time Cost}
    % \label{fig:sample}
\end{figure}

From the figure we can see that, our SA-CLOCK is a little slower than the LRU policy. However, when compared to the CLOCK policy, our SA-CLOCK has a great improvement. The overall time cost is reduced by 461 milliseconds, which is 28.3 percent. Since our SA-CLOCK is designed and implemented entirely based on CLOCK, this reduction in time cost shows that the size-aware admission policy significantly improved the cache's performance in relation to time efficiency. We believe that this is due to the fact that our SA-CLOCK rejects a number of objects that are too big, which in turn reduced the time spent on going around the clock structure used to keep track of recency information. In the meantime, this also means that the disadvantage compared to LRU can be explained by the nature of CLOCK-based algorithm, which is naturally slower than LRU.


\subsubsection{Memory Usage}
The most important aspect that our Size Aware CLOCK address is the memory usage of the cache. Based on CLOCK, whose implementation is simpler and is more memory efficient, our Size Aware CLOCK future manage memory usage by adding the size aware adoption policy. Therefore, we hope our Size-Aware CLOCK can significantly reduce memory usage. Figure 3 shows the average memory usage of each type of cache policy throughout the experiment process. Figure 4 shows the memory usage curve for each type of cache policy.

\begin{figure}[h]
\centerline{\includegraphics[width=0.5\textwidth]{pic/1000000-1000-mem}}
\caption{Average Memory Usage}
\label{fig}
\end{figure}

\begin{figure}[h]
\centerline{\includegraphics[width=0.5\textwidth]{pic/mem_records.png}}
\caption{Memory Usage Curve}
\label{fig}
\end{figure}

From figure 3, we can see that the average memory usage of LRU and CLOCK are very close. This is reasonable because they will always adopt any object given to them since neither of them has any policies that will reject an object. The nuanced difference is caused by the difference between their eviction policy, where LRU will evict precisely the least recent used algorithm, while CLOCK may be not so precise due to the clock structure it used. Compared to these two baseline models, SA-CLOCK has a much smaller memory usage in average. It only consume less than 32 percent of the memory used by the other two baseline models. This result shows that our SA-CLOCK can indeed reduce the memory usage in recency based cache algorithm by a lot.  
From figure 4, we can see the memory usage curve for each type of cache policy throughout the experiment process. Again, LRU and CLOCK have almost identical curves since they are very much alike in relation to memory usage. Our SA-CLOCK has kept a much smaller memory usage throughout the entire experiment. What is more, LRU and CLOCK both have a lot of peaks where the memory usage goes up drastically and then go back to average level. SA-CLOCK only has 2 peaks throughout the entire experiment. Memory usage peaks can be really harmful in production environments, where errors like OOM or decrease in performance due to memory recollection can happen and harm the application's overall stability. With much less memory peaks, our SA-CLOCK can help applications achieve better performance and stability in production environments.


\subsection{Rejection Analysis}
In this section, we analyze the rejects decisions made by our SA-CLOCK cache throughout the experiment. 

\begin{figure}[h]
\centerline{\includegraphics[width=0.5\textwidth]{pic/1000000-1000-rej}}
\caption{Number of Rejects}
\label{fig}
\end{figure}


\begin{figure}[h]
\centerline{\includegraphics[width=0.5\textwidth]{pic/rej.png}}
\caption{Rejected Objects Size}
\label{fig}
\end{figure}

Figure 5 shows the number of rejections on admitting new objects into the cache made by each type of cache policy. LRU and CLOCK both has 0 rejects as they do not have a admission policy that would reject objects. Our SA-CLOCK made 149904 rejections in total, which is about 15 percent of the total objects. With these rejections, our SA-CLOCK successfully reduced the total memory usage while not decreasing the object hit ratio too much.
Figure 6 shows the rejected objects' size throughout the experiment. From the figure, you can see that there are several peaks in the rejected objects' size, which can be matched to the memory usage peak of LRU and CLOCK in figure 4. This shows that our SA-CLOCK has successfully avoided several possible memory surges by using its threshold, and these valuable rejections has reduced the number of memory usage peaks that have taken place. 


% \subsection{Adaption to Change of Object Size}
% In this section, we con
\subsection{Parameter Tunning}
In this section, we do some experiments on tunning the parameter in our Size-Aware CLOCK, which is the only parameter that requires user to specify. Here we refer the parameter as $\alpha$. We tried 19 different value from 0.05 to 0.95 and recorded their performance metrics.

\begin{figure}
  \centering
  \begin{subfigure}[h]{0.4\textwidth}
    \includegraphics[width=\textwidth]{pic/alpha/miss.png}
    \caption{Miss Ratio}
    \label{fig:sub1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[h]{0.4\textwidth}
    \includegraphics[width=\textwidth]{pic/alpha/mem.png}
    \caption{Memory Usage}
    \label{fig:sub2}
  \end{subfigure}
  
  \medskip
  
  \begin{subfigure}[h]{0.4\textwidth}
    \includegraphics[width=\textwidth]{pic/alpha/tc.png}
    \caption{Time Cost}
    \label{fig:sub3}
  \end{subfigure}
  \hfill
  \begin{subfigure}[h]{0.4\textwidth}
    \includegraphics[width=\textwidth]{pic/alpha/rej.png}
    \caption{Rejections}
    \label{fig:sub4}
  \end{subfigure}
  
  \caption{Evaluation of Different Parameter}
  \label{fig:main} 
\end{figure}

Figure 7 shows the miss ratio, memory usage, time cost, and number of rejections of each value of $\alpha$.
From the miss ratio figure, we can see that the miss ratio varies very little for different choice of $\alpha$ in our experiment. Generally, 0.1 is the best one but the margins are very small and do not show any trend as $\alpha$ grows bigger.
From the memory usage figure, we can tell that as $\alpha$ grows bigger, the memory usage generally grows. This is reasonable because when $\alpha$ is bigger, our Size-Aware CLOCK reacts faster to the surge of the size of input objects. Therefore, when the input objects get bigger, a bigger $\alpha$ will enlarge the threshold quicker allow more large objects to enter the cache, resulting in a larger memory usage.
From the time cost figure, we can tell that as $\alpha$ changes, the time cost does not change very much. This is consistent with the result of miss ratio, as the time are mostly spent on replacing cached objects and only a cache miss would cause such operation.
From the rejection figure, we can see that as $\alpha$ grows, our Size-Aware CLOCK makes more rejections. This shows that although a bigger $\alpha$ reacts more quickly when the input objects' size increases, it also shrinks the threshold quicker when the objects' size decreases. This causes the threshold to be unstable and makes more rejections on our dataset.
In conclusion, the choice of $\alpha$ definitely has a great influence on the performance of our Size-Aware CLOCK. However, different $\alpha$ should be adopted for different datasets in order to reach maximum performance.


\section{Future Improvements}
From our experiments, we can see that our Size-Aware CLOCK has the ability to reduce cache memory usage, which is exactly our target. Nevertheless, there are still aspects that it can be further refined. 

The most significant aspect that needs refinement is the dynamic mechanism for our admission threshold. The threshold is the core of the admission policy and its current update mechanism is still immature. The adaption of the threshold rely heavily on the parameter $\alpha$, which needs to be chosen by the users. A suitable choice of $\alpha$ depends on users' previous knowledge about the dataset, which in turn makes users' burden heavier and it is unlikely that users can always make the perfect decision. Therefore, improvements could be made on the adjusting process of the threshold, especially ones that can make the threshold more self-adaptive and requires less human efforts.

\section{Conclusion}
In conclusion, our Size-Aware CLOCK, built upon the CLOCK algorithm with the addition of size-based rejection and dynamic threshold adjustment using an exponential smoothing algorithm, has demonstrated significant effectiveness in reducing memory consumption. The rejection of objects exceeding a dynamically adjusted threshold helps control the cache size, contributing to a more efficient memory usage. Moreover, it also notably improved the time efficiency on the base of CLOCK policy. On the downside, our experiments revealed a trade-off, as the cache hit rate experienced a slight degradation.

As for the ease-of-use of our Size-Aware CLOCK, the delicate balance required in adjusting the parameter of the exponential smoothing algorithm poses pressure on the users. Fine-tuning these parameters is crucial to optimizing the cache policy's performance and achieving a better trade-off between memory usage and cache hit rate. An improvement that makes the parameters self-adaptive could be a future path for enhancing the current policy.

\section*{References}

Please number citations consecutively within brackets \cite{b1}. The
sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference
number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at
the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

Number footnotes separately in superscripts. Place the actual footnote at
the bottom of the column in which it was cited. Do not put footnotes in the
abstract or reference list. Use letters for table footnotes.

Unless there are six authors or more give all authors' names; do not use
``et al.''. Papers that have not been published, even if they have been
submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers
that have been accepted for publication should be cited as ``in press'' \cite{b5}.
Capitalize only the first word in a paper title, except for proper nouns and
element symbols.

For papers published in translation journals, please give the English
citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
	\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
	\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
	\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
	\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
	\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
	\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
	\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}
\vspace{12pt}
\color{red}
IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
